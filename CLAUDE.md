# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## What This Is

TriAI Compare is a multi-AI response comparison platform built with Flask. It sends queries to 4 AI models (GPT-4o, Claude Sonnet 4, Gemini 2.0 Flash, Perplexity Sonar Pro) in parallel and displays side-by-side results with consensus synthesis, truth enforcement, and credibility scoring.

## Commands

```bash
# Setup
python -m venv venv && venv\Scripts\activate   # Windows
pip install -r requirements.txt
cp .env.example .env   # Then fill in API keys

# Run (development)
python app.py   # Serves on http://localhost:5000

# Run (production)
gunicorn app:app --bind 0.0.0.0:$PORT --timeout 120

# Test API connections
python test_apis.py

# Test workflow engine
python test_workflow.py
```

No formal test framework (pytest/unittest), linter, or formatter is configured.

## Architecture

### Backend Modules

- **`app.py`** — Main Flask app (~1800 lines). Contains all route handlers, the four `query_*()` functions (one per AI model), consensus generation, and the `/api/ask` endpoint that orchestrates parallel queries via `ThreadPoolExecutor`.
- **`database.py`** — SQLite3 layer (`comparisons.db`). Tables: `comparisons`, `responses`, `query_feedback`, `response_evaluations`. Schema auto-initializes on first run with migration support.
- **`council_roles.py`** — Role definitions for Council Mode. Each role (Liquidator, Integrity Auditor, Crisis Manager, etc.) has a system prompt, truth contract, forbidden terms, and auto-interrogation triggers. Imported as `COUNCIL_ROLES` dict.
- **`workflows.py`** — Multi-step agentic workflow engine. `Workflow` class chains sequential AI calls with context accumulation. Templates defined in `WORKFLOW_TEMPLATES`. Runs in background threads with status polling via `WORKFLOW_JOBS` dict in app.py.
- **`enforcement.py`** — `EnforcementEngine` class. Post-response auditing that detects fluff words, unanchored metrics, forbidden term violations, and credibility issues. Produces scores; triggers interrogation if score < 70.
- **`visuals.py`** — Flask Blueprint for visual generation. Mermaid diagrams (via Gemini) and fabricated images (via Google image generation).
- **`file_processor.py`** — Handles PDF (PyPDF2), image (Pillow + base64), and text file processing for document-attached queries.
- **`project_manager.py`** — JSON-file-based project/session persistence in `triai_projects/` directory.
- **`persona_synthesizer.py`** — Analyzes persona drift across model responses over time.
- **`feedback_analyzer.py`** — Gemini-powered feedback text classification.
- **`deployment_platforms.py`** — IaC template generation for Vercel, Render, Railway, Terraform.

### Frontend

Single-page app with no build step:
- **`templates/index.html`** — Main UI template
- **`templates/dashboard.html`** — Analytics dashboard (BasicAuth protected)
- **`static/app.js`** — All frontend logic (~3000 lines). Manages query submission, response rendering (4-column cards), Council/Hard mode toggles, workflow UI, interrogation, project/history sidebars.
- **`static/style.css`** + `static/css/*.css` — Dark theme with gold accents ("Gold Noir")

### Key API Endpoints

| Endpoint | Method | Purpose |
|---|---|---|
| `/api/ask` | POST | Main query — parallel AI requests + consensus |
| `/api/history` | GET | Fetch comparison history |
| `/api/feedback` | POST | Submit user feedback on comparisons |
| `/api/recommend_roles` | POST | Auto-recommend Council roles for a query |
| `/api/workflows` | GET | List available workflow templates |
| `/api/workflow/run` | POST | Execute a multi-step workflow (async) |
| `/api/workflow/status/<job_id>` | GET | Poll workflow progress |
| `/interrogate` | POST | Deep-dive interrogation of flagged claims |
| `/api/resynthesize` | POST | Re-evaluate consensus with enforcement |
| `/api/projects` | GET/POST | List or create projects |
| `/api/save_to_obsidian` | POST | Export comparison to Obsidian vault |
| `/visualize` | POST | Generate Mermaid/image visualizations |
| `/dashboard` | GET | Analytics dashboard (auth required) |

### Query Flow

1. `/api/ask` receives question + optional files + mode flags (council, hard, visual profile)
2. Four `query_*()` functions run in parallel via `ThreadPoolExecutor`
3. Each function builds a model-specific prompt with role/hard-mode directives, calls the API, then runs `enforcement_engine` on the response
4. Results saved to SQLite via `save_comparison()`
5. Consensus generated by sending all 4 responses to GPT-4o as "Chairman"
6. Frontend renders response cards with credibility badges, violation highlights, and thinking-tag extraction

### Environment Variables

```
OPENAI_API_KEY      # Required
ANTHROPIC_API_KEY   # Required
GOOGLE_API_KEY      # Required
PERPLEXITY_API_KEY  # Required
AUTH_USER            # Optional (default: admin)
AUTH_PASS            # Optional (default: triai2026)
OBSIDIAN_VAULT_PATH  # Optional, for Obsidian export
```

### SDK Notes

- **Google GenAI**: Uses `google-genai` SDK (`from google import genai`), NOT the deprecated `google-generativeai` package. The old SDK reached EOL Nov 2025.
- **Anthropic**: Uses `anthropic.Anthropic` client directly (no streaming).
- **OpenAI**: Standard `openai` SDK.
- **Perplexity**: Raw REST API via `requests` (no SDK).

### Database

SQLite3 at `comparisons.db`. Auto-creates tables on startup via `init_database()`. Column migrations handled with try/except `ALTER TABLE` pattern. The DB file is in `.gitignore` but may appear in the repo from development.
